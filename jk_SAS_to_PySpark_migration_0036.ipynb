{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mXvZ-LNbAox1"
      },
      "outputs": [],
      "source": [
        "# Installation and Imports\n",
        "%%capture\n",
        "!pip install pyspark pytest pandas numpy scipy scikit-learn\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import Imputer, VectorAssembler\n",
        "from pyspark.ml.stat import Correlation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Optional, Union, Tuple\n",
        "import unittest\n",
        "import pytest\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Health Data Analysis\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Function Definitions For SAS Operations\n",
        "class SASPySparkMigration:\n",
        "    \"\"\" SAS to PySpark migration\"\"\"\n",
        "\n",
        "    def __init__(self, df: pyspark.sql.DataFrame):\n",
        "        self.df = df\n",
        "\n",
        "    def sort_data(self,\n",
        "                 columns: List[str],\n",
        "                 ascending: Union[bool, List[bool]] = True) -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC SORT DATA=dataset;\n",
        "            BY col1 col2 ... coln;\n",
        "        RUN;\n",
        "\n",
        "        For descending sort:\n",
        "        PROC SORT DATA=dataset;\n",
        "            BY DESCENDING col1 DESCENDING col2;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        if isinstance(ascending, bool):\n",
        "            ascending = [ascending] * len(columns)\n",
        "\n",
        "        order_cols = [F.col(c).asc() if asc else F.col(c).desc()\n",
        "                     for c, asc in zip(columns, ascending)]\n",
        "        return self.df.orderBy(*order_cols)\n",
        "\n",
        "    def calculate_means(self,\n",
        "                       group_cols: List[str],\n",
        "                       analysis_cols: List[str]) -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC MEANS DATA=dataset MEAN STD MIN MAX N;\n",
        "            CLASS group_col1 group_col2;\n",
        "            VAR analysis_col1 analysis_col2;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        aggs = []\n",
        "        for col in analysis_cols:\n",
        "            aggs.extend([\n",
        "                F.mean(col).alias(f\"{col}_mean\"),\n",
        "                F.stddev(col).alias(f\"{col}_stddev\"),\n",
        "                F.min(col).alias(f\"{col}_min\"),\n",
        "                F.max(col).alias(f\"{col}_max\"),\n",
        "                F.count(col).alias(f\"{col}_count\")\n",
        "            ])\n",
        "\n",
        "        return self.df.groupBy(*group_cols).agg(*aggs)\n",
        "\n",
        "    def frequency_analysis(self,\n",
        "                         columns: List[str],\n",
        "                         include_pct: bool = True) -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC FREQ DATA=dataset;\n",
        "            TABLES col1 col2 / NOCUM NOPERCENT;\n",
        "        RUN;\n",
        "\n",
        "        With percentages:\n",
        "        PROC FREQ DATA=dataset;\n",
        "            TABLES col1 col2 / NOCUM;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        freq_df = self.df.groupBy(*columns).count()\n",
        "\n",
        "        if include_pct:\n",
        "            total = self.df.count()\n",
        "            freq_df = freq_df.withColumn(\n",
        "                \"percentage\",\n",
        "                F.round(F.col(\"count\") * 100 / total, 2)\n",
        "            )\n",
        "\n",
        "        return freq_df.orderBy(\"count\", ascending=False)\n",
        "\n",
        "    def univariate_analysis(self,\n",
        "                          numeric_col: str) -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC UNIVARIATE DATA=dataset;\n",
        "            VAR numeric_col;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        quantiles = self.df.approxQuantile(\n",
        "            numeric_col,\n",
        "            [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99],\n",
        "            0.01\n",
        "        )\n",
        "\n",
        "        stats_df = self.df.select(\n",
        "            F.mean(numeric_col).alias(\"mean\"),\n",
        "            F.stddev(numeric_col).alias(\"stddev\"),\n",
        "            F.skewness(numeric_col).alias(\"skewness\"),\n",
        "            F.kurtosis(numeric_col).alias(\"kurtosis\"),\n",
        "            F.min(numeric_col).alias(\"min\"),\n",
        "            F.max(numeric_col).alias(\"max\")\n",
        "        )\n",
        "\n",
        "        stats_pd = stats_df.toPandas()\n",
        "        quantiles_pd = pd.DataFrame([quantiles],\n",
        "                                  columns=['p1', 'p5', 'p10', 'p25', 'p50',\n",
        "                                         'p75', 'p90', 'p95', 'p99'])\n",
        "\n",
        "        return spark.createDataFrame(\n",
        "            pd.concat([stats_pd, quantiles_pd], axis=1)\n",
        "        )\n",
        "\n",
        "    def cross_tabulation(self,\n",
        "                        row_col: str,\n",
        "                        col_col: str,\n",
        "                        normalize: bool = False) -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        Creates a cross-tabulation (contingency table) of two columns.\n",
        "\n",
        "        Args:\n",
        "            row_col (str): Column name to use for rows\n",
        "            col_col (str): Column name to use for columns\n",
        "            normalize (bool): If True, adds percentage columns for each value\n",
        "\n",
        "        Returns:\n",
        "            pyspark.sql.DataFrame: Cross-tabulation result\n",
        "\n",
        "        Example:\n",
        "            >>> analyzer = DataFrameAnalyzer(medical_df)\n",
        "            >>> result = analyzer.cross_tabulation('diagnosis', 'gender', normalize=True)\n",
        "\n",
        "        SAS equivalent:\n",
        "        PROC TABULATE DATA=dataset;\n",
        "            CLASS row_col col_col;\n",
        "            TABLE row_col, col_col;\n",
        "        RUN;\n",
        "\n",
        "        With percentages:\n",
        "        PROC TABULATE DATA=dataset;\n",
        "            CLASS row_col col_col;\n",
        "            TABLE row_col, col_col / PTOC;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        # Create basic crosstab\n",
        "        crosstab = self.df.crosstab(row_col, col_col)\n",
        "\n",
        "        if normalize:\n",
        "            # Calculate column totals using window function\n",
        "            window = Window.partitionBy()\n",
        "            value_columns = crosstab.columns[1:]  # Skip the row label column\n",
        "\n",
        "            for col in value_columns:\n",
        "                # Calculate total for the column\n",
        "                total = F.sum(F.col(col)).over(window)\n",
        "                # Add percentage column\n",
        "                crosstab = crosstab.withColumn(\n",
        "                    f\"{col}_pct\",\n",
        "                    F.round(F.col(col) * 100 / total, 2)\n",
        "                )\n",
        "\n",
        "        return crosstab\n",
        "\n",
        "    def rank_data(self,\n",
        "                 rank_col: str,\n",
        "                 partition_cols: Optional[List[str]] = None,\n",
        "                 method: str = 'dense') -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC RANK DATA=dataset OUT=ranked_data TIES=DENSE;\n",
        "            BY partition_col1 partition_col2;\n",
        "            VAR rank_col;\n",
        "            RANKS rank_col_rank;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        window_spec = Window.partitionBy(*partition_cols if partition_cols else [])\n",
        "\n",
        "        rank_methods = {\n",
        "            'dense': F.dense_rank(),\n",
        "            'row_number': F.row_number(),\n",
        "            'percent': F.percent_rank()\n",
        "        }\n",
        "\n",
        "        return self.df.withColumn(\n",
        "            f\"{rank_col}_rank\",\n",
        "            rank_methods[method].over(window_spec.orderBy(rank_col))\n",
        "        )\n",
        "\n",
        "    def format_values(self,\n",
        "                     column: str,\n",
        "                     format_dict: Dict[str, str]) -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC FORMAT;\n",
        "            VALUE $formatname\n",
        "                'old_value1' = 'new_value1'\n",
        "                'old_value2' = 'new_value2';\n",
        "        RUN;\n",
        "\n",
        "        DATA new_dataset;\n",
        "            SET dataset;\n",
        "            formatted_col = PUT(column, $formatname.);\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        return self.df.replace(format_dict, subset=[column])\n",
        "\n",
        "    def transpose_data(self,\n",
        "                      id_cols: List[str],\n",
        "                      pivot_col: str,\n",
        "                      value_col: str) -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC TRANSPOSE DATA=dataset OUT=transposed_data;\n",
        "            BY id_col1 id_col2;\n",
        "            ID pivot_col;\n",
        "            VAR value_col;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        return self.df.groupBy(id_cols).pivot(pivot_col).agg(\n",
        "            F.first(value_col)\n",
        "        )\n",
        "\n",
        "    def merge_datasets(self,\n",
        "                      right_df: pyspark.sql.DataFrame,\n",
        "                      on: List[str],\n",
        "                      how: str = 'inner') -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        DATA merged_data;\n",
        "            MERGE dataset1(IN=a) dataset2(IN=b);\n",
        "            BY key1 key2;\n",
        "            IF a AND b;  /* For inner join */\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        return self.df.join(right_df, on=on, how=how)\n",
        "\n",
        "    def sql_query(self, query: str) -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC SQL;\n",
        "            SELECT *\n",
        "            FROM dataset\n",
        "            WHERE condition;\n",
        "        QUIT;\n",
        "        \"\"\"\n",
        "        self.df.createOrReplaceTempView(\"temp_table\")\n",
        "        return spark.sql(query)\n",
        "\n",
        "    def correlation_analysis(self,\n",
        "                           columns: List[str],\n",
        "                           method: str = 'pearson') -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC CORR DATA=dataset PEARSON;\n",
        "            VAR col1 col2 col3;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        vector_col = \"correlation_features\"\n",
        "        assembler = VectorAssembler(\n",
        "            inputCols=columns,\n",
        "            outputCol=vector_col,\n",
        "            handleInvalid=\"skip\"\n",
        "        )\n",
        "\n",
        "        vector_df = assembler.transform(self.df)\n",
        "        correlation_matrix = Correlation.corr(vector_df, vector_col, method)\n",
        "\n",
        "        return correlation_matrix\n",
        "\n",
        "    def standardize_data(self,\n",
        "                        numeric_cols: List[str],\n",
        "                        method: str = 'zscore') -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC STDIZE DATA=dataset METHOD=STD OUT=standardized_data;\n",
        "            VAR col1 col2;\n",
        "        RUN;\n",
        "\n",
        "        For min-max scaling:\n",
        "        PROC STDIZE DATA=dataset METHOD=RANGE OUT=standardized_data;\n",
        "            VAR col1 col2;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        result = self.df\n",
        "        for col in numeric_cols:\n",
        "            if method == 'zscore':\n",
        "                mean = self.df.select(F.mean(col)).collect()[0][0]\n",
        "                stddev = self.df.select(F.stddev(col)).collect()[0][0]\n",
        "                result = result.withColumn(\n",
        "                    f\"{col}_standardized\",\n",
        "                    (F.col(col) - mean) / stddev\n",
        "                )\n",
        "            elif method == 'minmax':\n",
        "                min_val = self.df.select(F.min(col)).collect()[0][0]\n",
        "                max_val = self.df.select(F.max(col)).collect()[0][0]\n",
        "                result = result.withColumn(\n",
        "                    f\"{col}_standardized\",\n",
        "                    (F.col(col) - min_val) / (max_val - min_val)\n",
        "                )\n",
        "        return result\n",
        "\n",
        "    def handle_missing(self,\n",
        "                      numeric_cols: List[str],\n",
        "                      method: str = 'mean') -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC STDIZE DATA=dataset METHOD=MEAN REPONLY;\n",
        "            VAR col1 col2;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        imputer = Imputer(\n",
        "            inputCols=numeric_cols,\n",
        "            outputCols=[f\"{c}_imputed\" for c in numeric_cols],\n",
        "            strategy=method\n",
        "        )\n",
        "        return imputer.fit(self.df).transform(self.df)\n",
        "\n",
        "    def rolling_window_analysis(self,\n",
        "                               column: str,\n",
        "                               window_size: int,\n",
        "                               agg_func: str = 'mean') -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC EXPAND DATA=dataset OUT=expanded_data;\n",
        "            ID time_col;\n",
        "            CONVERT column / TRANSFORMOUT=(MOVAVE window_size);\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        window_spec = Window.orderBy().rowsBetween(-window_size + 1, 0)\n",
        "        agg_funcs = {\n",
        "            'mean': F.mean,\n",
        "            'sum': F.sum,\n",
        "            'min': F.min,\n",
        "            'max': F.max\n",
        "        }\n",
        "\n",
        "        if agg_func not in agg_funcs:\n",
        "            raise ValueError(f\"Unsupported aggregation function: {agg_func}\")\n",
        "\n",
        "        return self.df.withColumn(\n",
        "            f\"{column}_rolling_{agg_func}_{window_size}\",\n",
        "            agg_funcs[agg_func](F.col(column)).over(window_spec)\n",
        "        )\n",
        "\n",
        "    def conditional_column_creation(self,\n",
        "                                    new_col: str,\n",
        "                                    conditions: Dict[str, str]) -> pyspark.sql.DataFrame:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        DATA new_dataset;\n",
        "            SET dataset;\n",
        "            IF condition1 THEN new_col = value1;\n",
        "            ELSE IF condition2 THEN new_col = value2;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        case_expr = F.when(F.lit(False), None)  # Placeholder for chaining\n",
        "        for condition, value in conditions.items():\n",
        "            case_expr = case_expr.when(F.expr(condition), value)\n",
        "\n",
        "        return self.df.withColumn(new_col, case_expr)\n",
        "\n",
        "    def summarize_dataset(self) -> None:\n",
        "        \"\"\"\n",
        "        Provides a summary of the dataset similar to SAS's PROC CONTENTS.\n",
        "        \"\"\"\n",
        "        print(\"Schema:\")\n",
        "        self.df.printSchema()\n",
        "        print(\"\\nSample Data:\")\n",
        "        self.df.show(5)\n",
        "        print(\"\\nRow Count:\", self.df.count())\n",
        "        print(\"\\nColumn Summary:\")\n",
        "        for col in self.df.columns:\n",
        "            self.df.select(\n",
        "                F.col(col).alias(\"column_value\")\n",
        "            ).groupBy(\"column_value\").count().show(5)\n",
        "\n",
        "    def split_dataset(self,\n",
        "                     ratio: float,\n",
        "                     seed: int = 42) -> (pyspark.sql.DataFrame, pyspark.sql.DataFrame):\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC SURVEYSELECT DATA=dataset OUT=sampled_data SAMPRATE=ratio;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        return self.df.randomSplit([ratio, 1 - ratio], seed)\n",
        "\n",
        "    def time_series_forecasting(self,\n",
        "                               date_col: str,\n",
        "                               value_col: str,\n",
        "                               periods: int) -> None:\n",
        "        \"\"\"\n",
        "        SAS equivalent:\n",
        "        PROC ARIMA DATA=dataset;\n",
        "            IDENTIFY VAR=value_col;\n",
        "            ESTIMATE Q=1 P=2;\n",
        "            FORECAST OUT=forecast_data LEAD=periods;\n",
        "        RUN;\n",
        "        \"\"\"\n",
        "        # Placeholder for implementing forecasting models (e.g., ARIMA with statsmodels)\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "Q3L6L5YHA9oY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Dataset Creation\n",
        "def create_health_dataset() -> pyspark.sql.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a sample healthcare dataset for testing and demonstration\n",
        "\n",
        "    Returns:\n",
        "        pyspark.sql.DataFrame: A DataFrame containing health data\n",
        "    \"\"\"\n",
        "    data = [\n",
        "        (\"P001\", \"Cancer\", 45, \"M\", \"ON\", 10.5, \"2024-01-01\", \"A\", 98.6, 120.5, 15),\n",
        "        (\"P002\", \"Diabetes\", 62, \"F\", \"BC\", 15.2, \"2024-01-02\", \"B\", 99.1, 135.2, 22),\n",
        "        (\"P003\", \"Heart Disease\", 55, \"M\", \"AB\", 12.8, \"2024-01-03\", \"A\", 98.9, 142.8, 18),\n",
        "        (\"P004\", \"Cancer\", 38, \"F\", \"ON\", 9.3, \"2024-01-04\", \"C\", None, 118.3, 12),\n",
        "        (\"P005\", \"Diabetes\", 71, \"M\", \"QC\", 14.7, \"2024-01-05\", \"B\", 99.3, None, 20),\n",
        "        (\"P006\", \"Heart Disease\", 48, \"F\", \"ON\", 11.2, \"2024-01-06\", \"A\", 98.8, 128.5, 16),\n",
        "        (\"P007\", \"Cancer\", 59, \"M\", \"BC\", 13.5, \"2024-01-07\", \"B\", None, 138.9, 19),\n",
        "        (\"P008\", \"Diabetes\", 42, \"F\", \"AB\", 8.9, \"2024-01-08\", \"C\", 99.0, 122.4, 14),\n",
        "        (\"P009\", \"Heart Disease\", 65, \"M\", \"QC\", 16.4, \"2024-01-09\", \"A\", 99.2, None, 25),\n",
        "        (\"P010\", \"Cancer\", 52, \"F\", \"ON\", 12.1, \"2024-01-10\", \"B\", 98.7, 130.6, 17)\n",
        "    ]\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"patient_id\", StringType(), False),\n",
        "        StructField(\"diagnosis\", StringType(), True),\n",
        "        StructField(\"age\", IntegerType(), True),\n",
        "        StructField(\"gender\", StringType(), True),\n",
        "        StructField(\"province\", StringType(), True),\n",
        "        StructField(\"length_of_stay\", DoubleType(), True),\n",
        "        StructField(\"admission_date\", StringType(), True),\n",
        "        StructField(\"care_level\", StringType(), True),\n",
        "        StructField(\"temperature\", DoubleType(), True),\n",
        "        StructField(\"blood_pressure\", DoubleType(), True),\n",
        "        StructField(\"visits\", IntegerType(), True)\n",
        "    ])\n",
        "\n",
        "    return spark.createDataFrame(data, schema)\n",
        "\n",
        "def create_supplementary_health_dataset() -> pyspark.sql.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a supplementary health dataset for testing merge-operations.\n",
        "    Contains additional patient information that complements the main dataset.\n",
        "\n",
        "    Returns:\n",
        "        pyspark.sql.DataFrame: A DataFrame containing supplementary health data\n",
        "    \"\"\"\n",
        "    data = [\n",
        "        (\"P001\", \"A+\", 14.2, \"Non-smoker\", \"Private\", 3),\n",
        "        (\"P002\", \"B-\", 13.8, \"Smoker\", \"Medicare\", 2),\n",
        "        (\"P003\", \"O+\", 15.1, \"Non-smoker\", \"Medicaid\", 4),\n",
        "        (\"P004\", \"AB+\", 12.9, \"Former smoker\", \"Private\", 1),\n",
        "        (\"P005\", \"A-\", 13.5, \"Smoker\", \"Uninsured\", 3),\n",
        "        (\"P006\", \"O-\", 14.8, \"Non-smoker\", \"Medicare\", 2),\n",
        "        (\"P007\", \"B+\", 13.2, \"Smoker\", \"Private\", 4),\n",
        "        (\"P008\", \"AB-\", 14.5, \"Non-smoker\", \"Medicaid\", 1),\n",
        "        (\"P009\", \"O+\", 13.7, \"Former smoker\", \"Private\", 3),\n",
        "        (\"P010\", \"A+\", 14.9, \"Non-smoker\", \"Medicare\", 2)\n",
        "    ]\n",
        "\n",
        "    schema = StructType([\n",
        "        StructField(\"patient_id\", StringType(), False),\n",
        "        StructField(\"blood_type\", StringType(), True),\n",
        "        StructField(\"hemoglobin\", DoubleType(), True),\n",
        "        StructField(\"smoking_status\", StringType(), True),\n",
        "        StructField(\"insurance_type\", StringType(), True),\n",
        "        StructField(\"follow_ups_scheduled\", IntegerType(), True)\n",
        "    ])\n",
        "\n",
        "    return spark.createDataFrame(data, schema)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "jC33qKiHp-Cr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Unit Tests\n",
        "class TestSASPySparkMigration(unittest.TestCase):\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        cls.df = create_health_dataset()\n",
        "        cls.migration = SASPySparkMigration(cls.df)\n",
        "        # Create a second DataFrame for merge-testing\n",
        "        cls.df2 = create_supplementary_health_dataset()\n",
        "\n",
        "    def test_sort_data(self):\n",
        "        \"\"\"Test PROC SORT equivalent\"\"\"\n",
        "        sorted_df = self.migration.sort_data([\"age\"])\n",
        "        ages = sorted_df.select(\"age\").collect()\n",
        "        self.assertTrue(all(ages[i][0] <= ages[i+1][0]\n",
        "                          for i in range(len(ages)-1)))\n",
        "\n",
        "    def test_calculate_means(self):\n",
        "        \"\"\"Test PROC MEANS equivalent\"\"\"\n",
        "        means_df = self.migration.calculate_means(\n",
        "            [\"diagnosis\"],\n",
        "            [\"length_of_stay\", \"visits\"]\n",
        "        )\n",
        "        self.assertTrue(means_df.count() > 0)\n",
        "        self.assertTrue(\"length_of_stay_mean\" in means_df.columns)\n",
        "        self.assertTrue(\"visits_mean\" in means_df.columns)\n",
        "\n",
        "    def test_frequency_analysis(self):\n",
        "        \"\"\"Test PROC FREQ equivalent\"\"\"\n",
        "        freq_df = self.migration.frequency_analysis([\"diagnosis\", \"gender\"])\n",
        "        self.assertTrue(\"percentage\" in freq_df.columns)\n",
        "        self.assertTrue(freq_df.count() > 0)\n",
        "\n",
        "    def test_univariate_analysis(self):\n",
        "        \"\"\"Test PROC UNIVARIATE equivalent\"\"\"\n",
        "        univ_df = self.migration.univariate_analysis(\"age\")\n",
        "        self.assertTrue(univ_df.count() == 1)\n",
        "        required_cols = [\"mean\", \"stddev\", \"skewness\", \"kurtosis\", \"p25\", \"p50\", \"p75\"]\n",
        "        self.assertTrue(all(col in univ_df.columns for col in required_cols))\n",
        "\n",
        "    def test_cross_tabulation(self):\n",
        "        \"\"\"Test PROC TABULATE equivalent\"\"\"\n",
        "        cross_df = self.migration.cross_tabulation(\"diagnosis\", \"gender\", normalize=True)\n",
        "        self.assertTrue(cross_df.count() > 0)\n",
        "        self.assertTrue(\"M_pct\" in cross_df.columns or \"F_pct\" in cross_df.columns)\n",
        "\n",
        "    def test_rank_data(self):\n",
        "        \"\"\"Test PROC RANK equivalent\"\"\"\n",
        "        ranked_df = self.migration.rank_data(\"length_of_stay\", [\"diagnosis\"])\n",
        "        self.assertTrue(\"length_of_stay_rank\" in ranked_df.columns)\n",
        "\n",
        "    def test_format_values(self):\n",
        "        \"\"\"Test PROC FORMAT equivalent\"\"\"\n",
        "        format_dict = {\"A\": \"High\", \"B\": \"Medium\", \"C\": \"Low\"}\n",
        "        formatted_df = self.migration.format_values(\"care_level\", format_dict)\n",
        "        unique_values = formatted_df.select(\"care_level\").distinct().collect()\n",
        "        self.assertTrue(all(row[0] in [\"High\", \"Medium\", \"Low\"] for row in unique_values))\n",
        "\n",
        "    def test_transpose_data(self):\n",
        "        \"\"\"Test PROC TRANSPOSE equivalent\"\"\"\n",
        "        trans_df = self.migration.transpose_data(\n",
        "            [\"patient_id\"],\n",
        "            \"diagnosis\",\n",
        "            \"length_of_stay\"\n",
        "        )\n",
        "        self.assertTrue(\"Cancer\" in trans_df.columns)\n",
        "        self.assertTrue(\"Diabetes\" in trans_df.columns)\n",
        "\n",
        "    def test_merge_datasets(self):\n",
        "        \"\"\"Test DATA MERGE equivalent\"\"\"\n",
        "        merged_df = self.migration.merge_datasets(\n",
        "            self.df2,\n",
        "            on=[\"patient_id\"],\n",
        "            how=\"inner\"\n",
        "        )\n",
        "        # Check if merged DataFrame has more columns than original\n",
        "        self.assertTrue(len(merged_df.columns) > len(self.df.columns))\n",
        "        # Check if common keys are preserved\n",
        "        self.assertTrue(\"patient_id\" in merged_df.columns)\n",
        "\n",
        "    def test_sql_query(self):\n",
        "        \"\"\"Test PROC SQL equivalent\"\"\"\n",
        "        query = \"SELECT COUNT(*) as count, diagnosis FROM temp_table GROUP BY diagnosis\"\n",
        "        sql_df = self.migration.sql_query(query)\n",
        "        # Check if query executed successfully\n",
        "        self.assertTrue(sql_df.count() > 0)\n",
        "        self.assertTrue(\"count\" in sql_df.columns)\n",
        "        self.assertTrue(\"diagnosis\" in sql_df.columns)\n",
        "\n",
        "    def test_correlation_analysis(self):\n",
        "        \"\"\"Test PROC CORR equivalent\"\"\"\n",
        "        corr_df = self.migration.correlation_analysis(\n",
        "            [\"age\", \"length_of_stay\", \"visits\"]\n",
        "        )\n",
        "        self.assertTrue(corr_df.count() > 0)\n",
        "        # Check if correlation matrix is symmetric\n",
        "        corr_matrix = corr_df.collect()[0][0].toArray()\n",
        "        self.assertTrue(np.allclose(corr_matrix, corr_matrix.T))\n",
        "\n",
        "    def test_standardize_data(self):\n",
        "        \"\"\"Test PROC STDIZE equivalent\"\"\"\n",
        "        std_df = self.migration.standardize_data(\n",
        "            [\"age\", \"length_of_stay\"],\n",
        "            method=\"zscore\"\n",
        "        )\n",
        "        self.assertTrue(\"age_standardized\" in std_df.columns)\n",
        "        self.assertTrue(\"length_of_stay_standardized\" in std_df.columns)\n",
        "\n",
        "        # Test min-max scaling\n",
        "        minmax_df = self.migration.standardize_data(\n",
        "            [\"age\"],\n",
        "            method=\"minmax\"\n",
        "        )\n",
        "        minmax_values = minmax_df.select(\"age_standardized\").collect()\n",
        "        self.assertTrue(all(0 <= row[0] <= 1 for row in minmax_values if row[0] is not None))\n",
        "\n",
        "    def test_handle_missing(self):\n",
        "        \"\"\"Test handling missing values\"\"\"\n",
        "        imputed_df = self.migration.handle_missing(\n",
        "            [\"temperature\", \"blood_pressure\"]\n",
        "        )\n",
        "        self.assertTrue(\"temperature_imputed\" in imputed_df.columns)\n",
        "        self.assertTrue(\"blood_pressure_imputed\" in imputed_df.columns)\n",
        "\n",
        "        # Check if no nulls remain in imputed columns\n",
        "        null_count = imputed_df.select(\n",
        "            [F.count(F.when(F.col(f\"{c}_imputed\").isNull(), c))\n",
        "             for c in [\"temperature\", \"blood_pressure\"]]\n",
        "        ).collect()[0]\n",
        "        self.assertTrue(all(count == 0 for count in null_count))"
      ],
      "metadata": {
        "id": "pYu4M-EYBQ_A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now run the tests\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=[''], verbosity=2, exit=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtZUpHXRjFIy",
        "outputId": "426d2647-6a77-4d49-9a21-b268d5f4928e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_calculate_means (__main__.TestSASPySparkMigration)\n",
            "Test PROC MEANS equivalent ... ok\n",
            "test_correlation_analysis (__main__.TestSASPySparkMigration)\n",
            "Test PROC CORR equivalent ... ok\n",
            "test_cross_tabulation (__main__.TestSASPySparkMigration)\n",
            "Test PROC TABULATE equivalent ... ok\n",
            "test_format_values (__main__.TestSASPySparkMigration)\n",
            "Test PROC FORMAT equivalent ... ok\n",
            "test_frequency_analysis (__main__.TestSASPySparkMigration)\n",
            "Test PROC FREQ equivalent ... ok\n",
            "test_handle_missing (__main__.TestSASPySparkMigration)\n",
            "Test handling missing values ... ok\n",
            "test_merge_datasets (__main__.TestSASPySparkMigration)\n",
            "Test DATA MERGE equivalent ... ok\n",
            "test_rank_data (__main__.TestSASPySparkMigration)\n",
            "Test PROC RANK equivalent ... ok\n",
            "test_sort_data (__main__.TestSASPySparkMigration)\n",
            "Test PROC SORT equivalent ... ok\n",
            "test_sql_query (__main__.TestSASPySparkMigration)\n",
            "Test PROC SQL equivalent ... ok\n",
            "test_standardize_data (__main__.TestSASPySparkMigration)\n",
            "Test PROC STDIZE equivalent ... ok\n",
            "test_transpose_data (__main__.TestSASPySparkMigration)\n",
            "Test PROC TRANSPOSE equivalent ... ok\n",
            "test_univariate_analysis (__main__.TestSASPySparkMigration)\n",
            "Test PROC UNIVARIATE equivalent ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 13 tests in 19.076s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_examples():\n",
        "    # Create sample dataset\n",
        "    health_df = create_health_dataset()\n",
        "    migration = SASPySparkMigration(health_df)\n",
        "\n",
        "    # Example 1: Sorting\n",
        "    print(\"Sorted Data:\")\n",
        "    sorted_df = migration.sort_data([\"diagnosis\", \"age\"], ascending=[True, False])\n",
        "    sorted_df.show()\n",
        "\n",
        "    # Example 2: Basic Statistics by Diagnosis\n",
        "    print(\"Basic Statistics for Length of Stay and Visits by Diagnosis:\")\n",
        "    migration.calculate_means(\n",
        "        [\"diagnosis\"],\n",
        "        [\"length_of_stay\", \"visits\"]\n",
        "    ).show()\n",
        "\n",
        "    # Example 3: Frequency Analysis with Cross Tabulation\n",
        "    print(\"\\nDiagnosis and Gender Distribution:\")\n",
        "    migration.cross_tabulation(\n",
        "        \"diagnosis\",\n",
        "        \"gender\",\n",
        "        normalize=True\n",
        "    ).show()\n",
        "\n",
        "    # Example 4: Univariate Analysis\n",
        "    print(\"\\nUnivariate Analysis of Age:\")\n",
        "    migration.univariate_analysis(\"age\").show()\n",
        "\n",
        "    # Example 5: Rank Patients by Visits\n",
        "    print(\"\\nRanking Patients by Visits:\")\n",
        "    migration.rank_data(\n",
        "        \"visits\",\n",
        "        partition_cols=[\"diagnosis\"],\n",
        "        method=\"dense\"\n",
        "    ).show()\n",
        "\n",
        "    # Example 6: Format Gender Values\n",
        "    print(\"\\nFormatted Gender Values:\")\n",
        "    format_dict = {\"M\": \"Male\", \"F\": \"Female\"}\n",
        "    migration.format_values(\"gender\", format_dict).show()\n",
        "\n",
        "    # Example 7: Transpose Data\n",
        "    print(\"\\nTransposed Data (Diagnosis as Pivot):\")\n",
        "    migration.transpose_data(\n",
        "        [\"patient_id\"],\n",
        "        \"diagnosis\",\n",
        "        \"length_of_stay\"\n",
        "    ).show()\n",
        "\n",
        "    # Example 8: Merge Datasets\n",
        "    print(\"\\nMerged Dataset:\")\n",
        "    other_df = create_supplementary_health_dataset()  # Assume this creates a compatible DataFrame\n",
        "    migration.merge_datasets(\n",
        "        other_df,\n",
        "        on=[\"patient_id\"],\n",
        "        how=\"inner\"\n",
        "    ).show()\n",
        "\n",
        "    # Example 9: SQL Analysis\n",
        "    print(\"\\nSQL Analysis of Average Stay by Diagnosis:\")\n",
        "    query = \"\"\"\n",
        "    SELECT diagnosis,\n",
        "           COUNT(*) as patient_count,\n",
        "           ROUND(AVG(length_of_stay), 2) as avg_stay,\n",
        "           ROUND(AVG(visits), 2) as avg_visits\n",
        "    FROM temp_table\n",
        "    GROUP BY diagnosis\n",
        "    ORDER BY avg_stay DESC\n",
        "    \"\"\"\n",
        "    migration.sql_query(query).show()\n",
        "\n",
        "    # Example 10: Correlation Analysis\n",
        "    print(\"\\nCorrelation Analysis:\")\n",
        "    migration.correlation_analysis(\n",
        "        [\"age\", \"length_of_stay\", \"visits\"]\n",
        "    ).show()\n",
        "\n",
        "    # Example 11: Standardize Values\n",
        "    print(\"\\nStandardized Values:\")\n",
        "    migration.standardize_data([\"age\", \"length_of_stay\"], method=\"zscore\").show()\n",
        "\n",
        "    # Example 12: Handle Missing Values\n",
        "    print(\"\\nHandle Missing Values (Impute with Mean):\")\n",
        "    migration.handle_missing([\"temperature\", \"blood_pressure\"], method=\"mean\").show()\n",
        "\n",
        "    # Example 13: Rolling Window Analysis\n",
        "    print(\"\\nRolling Window Analysis (7-day moving average of temperature):\")\n",
        "    migration.rolling_window_analysis(\n",
        "        \"temperature\",\n",
        "        window_size=7,\n",
        "        agg_func=\"mean\"\n",
        "    ).show()\n",
        "\n",
        "    # Example 14: Conditional Column Creation\n",
        "    print(\"\\nConditional Column Creation:\")\n",
        "    conditions = {\n",
        "        \"age < 18\": \"'Minor'\",\n",
        "        \"age >= 18 AND age < 65\": \"'Adult'\",\n",
        "        \"age >= 65\": \"'Senior'\"\n",
        "    }\n",
        "    migration.conditional_column_creation(\"age_group\", conditions).show()\n",
        "\n",
        "    # Example 15: Dataset Summary\n",
        "    print(\"\\nDataset Summary:\")\n",
        "    migration.summarize_dataset()\n",
        "\n",
        "    # Example 16: Split Dataset\n",
        "    print(\"\\nSplit Dataset into Training and Testing:\")\n",
        "    train_df, test_df = migration.split_dataset(ratio=0.8)\n",
        "    print(\"Training Data Sample:\")\n",
        "    train_df.show(5)\n",
        "    print(\"Testing Data Sample:\")\n",
        "    test_df.show(5)\n",
        "\n",
        "    # Example 17: Time Series Forecasting\n",
        "    print(\"\\nTime Series Forecasting (placeholder):\")\n",
        "    migration.time_series_forecasting(\"date\", \"length_of_stay\", periods=10)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run examples\n",
        "    run_examples()\n",
        "\n",
        "    # Run unittest test suite\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBn4d4Yu2j0e",
        "outputId": "fd2ea296-1fab-49d0-e0bc-ad91ce6bb792"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted Data:\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "|patient_id|    diagnosis|age|gender|province|length_of_stay|admission_date|care_level|temperature|blood_pressure|visits|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "|      P007|       Cancer| 59|     M|      BC|          13.5|    2024-01-07|         B|       NULL|         138.9|    19|\n",
            "|      P010|       Cancer| 52|     F|      ON|          12.1|    2024-01-10|         B|       98.7|         130.6|    17|\n",
            "|      P001|       Cancer| 45|     M|      ON|          10.5|    2024-01-01|         A|       98.6|         120.5|    15|\n",
            "|      P004|       Cancer| 38|     F|      ON|           9.3|    2024-01-04|         C|       NULL|         118.3|    12|\n",
            "|      P005|     Diabetes| 71|     M|      QC|          14.7|    2024-01-05|         B|       99.3|          NULL|    20|\n",
            "|      P002|     Diabetes| 62|     F|      BC|          15.2|    2024-01-02|         B|       99.1|         135.2|    22|\n",
            "|      P008|     Diabetes| 42|     F|      AB|           8.9|    2024-01-08|         C|       99.0|         122.4|    14|\n",
            "|      P009|Heart Disease| 65|     M|      QC|          16.4|    2024-01-09|         A|       99.2|          NULL|    25|\n",
            "|      P003|Heart Disease| 55|     M|      AB|          12.8|    2024-01-03|         A|       98.9|         142.8|    18|\n",
            "|      P006|Heart Disease| 48|     F|      ON|          11.2|    2024-01-06|         A|       98.8|         128.5|    16|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "\n",
            "Basic Statistics for Length of Stay and Visits by Diagnosis:\n",
            "+-------------+-------------------+---------------------+------------------+------------------+--------------------+------------------+------------------+----------+----------+------------+\n",
            "|    diagnosis|length_of_stay_mean|length_of_stay_stddev|length_of_stay_min|length_of_stay_max|length_of_stay_count|       visits_mean|     visits_stddev|visits_min|visits_max|visits_count|\n",
            "+-------------+-------------------+---------------------+------------------+------------------+--------------------+------------------+------------------+----------+----------+------------+\n",
            "|Heart Disease| 13.466666666666667|   2.6633312473917568|              11.2|              16.4|                   3|19.666666666666668| 4.725815626252608|        16|        25|           3|\n",
            "|     Diabetes| 12.933333333333332|   3.5019042438840797|               8.9|              15.2|                   3|18.666666666666668| 4.163331998932266|        14|        22|           3|\n",
            "|       Cancer| 11.350000000000001|   1.8357559750685821|               9.3|              13.5|                   4|             15.75|2.9860788111948193|        12|        19|           4|\n",
            "+-------------+-------------------+---------------------+------------------+------------------+--------------------+------------------+------------------+----------+----------+------------+\n",
            "\n",
            "\n",
            "Diagnosis and Gender Distribution:\n",
            "+----------------+---+---+-----+-----+\n",
            "|diagnosis_gender|  F|  M|F_pct|M_pct|\n",
            "+----------------+---+---+-----+-----+\n",
            "|   Heart Disease|  1|  2| 20.0| 40.0|\n",
            "|        Diabetes|  2|  1| 40.0| 20.0|\n",
            "|          Cancer|  2|  2| 40.0| 40.0|\n",
            "+----------------+---+---+-----+-----+\n",
            "\n",
            "\n",
            "Univariate Analysis of Age:\n",
            "+----+------------------+-------------------+------------------+---+---+----+----+----+----+----+----+----+----+----+\n",
            "|mean|            stddev|           skewness|          kurtosis|min|max|  p1|  p5| p10| p25| p50| p75| p90| p95| p99|\n",
            "+----+------------------+-------------------+------------------+---+---+----+----+----+----+----+----+----+----+----+\n",
            "|53.7|10.646334789233544|0.09932631337832339|-1.100083267985869| 38| 71|38.0|38.0|38.0|45.0|52.0|62.0|65.0|71.0|71.0|\n",
            "+----+------------------+-------------------+------------------+---+---+----+----+----+----+----+----+----+----+----+\n",
            "\n",
            "\n",
            "Ranking Patients by Visits:\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+-----------+\n",
            "|patient_id|    diagnosis|age|gender|province|length_of_stay|admission_date|care_level|temperature|blood_pressure|visits|visits_rank|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+-----------+\n",
            "|      P004|       Cancer| 38|     F|      ON|           9.3|    2024-01-04|         C|       NULL|         118.3|    12|          1|\n",
            "|      P001|       Cancer| 45|     M|      ON|          10.5|    2024-01-01|         A|       98.6|         120.5|    15|          2|\n",
            "|      P010|       Cancer| 52|     F|      ON|          12.1|    2024-01-10|         B|       98.7|         130.6|    17|          3|\n",
            "|      P007|       Cancer| 59|     M|      BC|          13.5|    2024-01-07|         B|       NULL|         138.9|    19|          4|\n",
            "|      P008|     Diabetes| 42|     F|      AB|           8.9|    2024-01-08|         C|       99.0|         122.4|    14|          1|\n",
            "|      P005|     Diabetes| 71|     M|      QC|          14.7|    2024-01-05|         B|       99.3|          NULL|    20|          2|\n",
            "|      P002|     Diabetes| 62|     F|      BC|          15.2|    2024-01-02|         B|       99.1|         135.2|    22|          3|\n",
            "|      P006|Heart Disease| 48|     F|      ON|          11.2|    2024-01-06|         A|       98.8|         128.5|    16|          1|\n",
            "|      P003|Heart Disease| 55|     M|      AB|          12.8|    2024-01-03|         A|       98.9|         142.8|    18|          2|\n",
            "|      P009|Heart Disease| 65|     M|      QC|          16.4|    2024-01-09|         A|       99.2|          NULL|    25|          3|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+-----------+\n",
            "\n",
            "\n",
            "Formatted Gender Values:\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "|patient_id|    diagnosis|age|gender|province|length_of_stay|admission_date|care_level|temperature|blood_pressure|visits|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "|      P001|       Cancer| 45|  Male|      ON|          10.5|    2024-01-01|         A|       98.6|         120.5|    15|\n",
            "|      P002|     Diabetes| 62|Female|      BC|          15.2|    2024-01-02|         B|       99.1|         135.2|    22|\n",
            "|      P003|Heart Disease| 55|  Male|      AB|          12.8|    2024-01-03|         A|       98.9|         142.8|    18|\n",
            "|      P004|       Cancer| 38|Female|      ON|           9.3|    2024-01-04|         C|       NULL|         118.3|    12|\n",
            "|      P005|     Diabetes| 71|  Male|      QC|          14.7|    2024-01-05|         B|       99.3|          NULL|    20|\n",
            "|      P006|Heart Disease| 48|Female|      ON|          11.2|    2024-01-06|         A|       98.8|         128.5|    16|\n",
            "|      P007|       Cancer| 59|  Male|      BC|          13.5|    2024-01-07|         B|       NULL|         138.9|    19|\n",
            "|      P008|     Diabetes| 42|Female|      AB|           8.9|    2024-01-08|         C|       99.0|         122.4|    14|\n",
            "|      P009|Heart Disease| 65|  Male|      QC|          16.4|    2024-01-09|         A|       99.2|          NULL|    25|\n",
            "|      P010|       Cancer| 52|Female|      ON|          12.1|    2024-01-10|         B|       98.7|         130.6|    17|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "\n",
            "\n",
            "Transposed Data (Diagnosis as Pivot):\n",
            "+----------+------+--------+-------------+\n",
            "|patient_id|Cancer|Diabetes|Heart Disease|\n",
            "+----------+------+--------+-------------+\n",
            "|      P007|  13.5|    NULL|         NULL|\n",
            "|      P003|  NULL|    NULL|         12.8|\n",
            "|      P010|  12.1|    NULL|         NULL|\n",
            "|      P006|  NULL|    NULL|         11.2|\n",
            "|      P004|   9.3|    NULL|         NULL|\n",
            "|      P002|  NULL|    15.2|         NULL|\n",
            "|      P001|  10.5|    NULL|         NULL|\n",
            "|      P008|  NULL|     8.9|         NULL|\n",
            "|      P009|  NULL|    NULL|         16.4|\n",
            "|      P005|  NULL|    14.7|         NULL|\n",
            "+----------+------+--------+-------------+\n",
            "\n",
            "\n",
            "Merged Dataset:\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+----------+----------+--------------+--------------+--------------------+\n",
            "|patient_id|    diagnosis|age|gender|province|length_of_stay|admission_date|care_level|temperature|blood_pressure|visits|blood_type|hemoglobin|smoking_status|insurance_type|follow_ups_scheduled|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+----------+----------+--------------+--------------+--------------------+\n",
            "|      P001|       Cancer| 45|     M|      ON|          10.5|    2024-01-01|         A|       98.6|         120.5|    15|        A+|      14.2|    Non-smoker|       Private|                   3|\n",
            "|      P002|     Diabetes| 62|     F|      BC|          15.2|    2024-01-02|         B|       99.1|         135.2|    22|        B-|      13.8|        Smoker|      Medicare|                   2|\n",
            "|      P003|Heart Disease| 55|     M|      AB|          12.8|    2024-01-03|         A|       98.9|         142.8|    18|        O+|      15.1|    Non-smoker|      Medicaid|                   4|\n",
            "|      P004|       Cancer| 38|     F|      ON|           9.3|    2024-01-04|         C|       NULL|         118.3|    12|       AB+|      12.9| Former smoker|       Private|                   1|\n",
            "|      P005|     Diabetes| 71|     M|      QC|          14.7|    2024-01-05|         B|       99.3|          NULL|    20|        A-|      13.5|        Smoker|     Uninsured|                   3|\n",
            "|      P006|Heart Disease| 48|     F|      ON|          11.2|    2024-01-06|         A|       98.8|         128.5|    16|        O-|      14.8|    Non-smoker|      Medicare|                   2|\n",
            "|      P007|       Cancer| 59|     M|      BC|          13.5|    2024-01-07|         B|       NULL|         138.9|    19|        B+|      13.2|        Smoker|       Private|                   4|\n",
            "|      P008|     Diabetes| 42|     F|      AB|           8.9|    2024-01-08|         C|       99.0|         122.4|    14|       AB-|      14.5|    Non-smoker|      Medicaid|                   1|\n",
            "|      P009|Heart Disease| 65|     M|      QC|          16.4|    2024-01-09|         A|       99.2|          NULL|    25|        O+|      13.7| Former smoker|       Private|                   3|\n",
            "|      P010|       Cancer| 52|     F|      ON|          12.1|    2024-01-10|         B|       98.7|         130.6|    17|        A+|      14.9|    Non-smoker|      Medicare|                   2|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+----------+----------+--------------+--------------+--------------------+\n",
            "\n",
            "\n",
            "SQL Analysis of Average Stay by Diagnosis:\n",
            "+-------------+-------------+--------+----------+\n",
            "|    diagnosis|patient_count|avg_stay|avg_visits|\n",
            "+-------------+-------------+--------+----------+\n",
            "|Heart Disease|            3|   13.47|     19.67|\n",
            "|     Diabetes|            3|   12.93|     18.67|\n",
            "|       Cancer|            4|   11.35|     15.75|\n",
            "+-------------+-------------+--------+----------+\n",
            "\n",
            "\n",
            "Correlation Analysis:\n",
            "+-----------------------------+\n",
            "|pearson(correlation_features)|\n",
            "+-----------------------------+\n",
            "|         1.0              ...|\n",
            "+-----------------------------+\n",
            "\n",
            "\n",
            "Standardized Values:\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+-------------------+---------------------------+\n",
            "|patient_id|    diagnosis|age|gender|province|length_of_stay|admission_date|care_level|temperature|blood_pressure|visits|   age_standardized|length_of_stay_standardized|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+-------------------+---------------------------+\n",
            "|      P001|       Cancer| 45|     M|      ON|          10.5|    2024-01-01|         A|       98.6|         120.5|    15|-0.8171826428751953|        -0.7756742945110732|\n",
            "|      P002|     Diabetes| 62|     F|      BC|          15.2|    2024-01-02|         B|       99.1|         135.2|    22|  0.779611027110818|         1.0843610035511948|\n",
            "|      P003|Heart Disease| 55|     M|      AB|          12.8|    2024-01-03|         A|       98.9|         142.8|    18|0.12210775123422428|        0.13455574496620729|\n",
            "|      P004|       Cancer| 38|     F|      ON|           9.3|    2024-01-04|         C|       NULL|         118.3|    12|-1.4746859187517891|        -1.2505769238035669|\n",
            "|      P005|     Diabetes| 71|     M|      QC|          14.7|    2024-01-05|         B|       99.3|          NULL|    20| 1.6249723818092956|         0.8864849080126556|\n",
            "|      P006|Heart Disease| 48|     F|      ON|          11.2|    2024-01-06|         A|       98.8|         128.5|    16|-0.5353955246423694|        -0.4986477607571186|\n",
            "|      P007|       Cancer| 59|     M|      BC|          13.5|    2024-01-07|         B|       NULL|         138.9|    19| 0.4978239088779921|        0.41158227872016184|\n",
            "|      P008|     Diabetes| 42|     F|      AB|           8.9|    2024-01-08|         C|       99.0|         122.4|    14|-1.0989697611080211|        -1.4088778002343982|\n",
            "|      P009|Heart Disease| 65|     M|      QC|          16.4|    2024-01-09|         A|       99.2|          NULL|    25| 1.0613981453436439|         1.5592636328436884|\n",
            "|      P010|       Cancer| 52|     F|      ON|          12.1|    2024-01-10|         B|       98.7|         130.6|    17|-0.1596793669986016|       -0.14247078878774797|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+-------------------+---------------------------+\n",
            "\n",
            "\n",
            "Handle Missing Values (Impute with Mean):\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+-------------------+----------------------+\n",
            "|patient_id|    diagnosis|age|gender|province|length_of_stay|admission_date|care_level|temperature|blood_pressure|visits|temperature_imputed|blood_pressure_imputed|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+-------------------+----------------------+\n",
            "|      P001|       Cancer| 45|     M|      ON|          10.5|    2024-01-01|         A|       98.6|         120.5|    15|               98.6|                 120.5|\n",
            "|      P002|     Diabetes| 62|     F|      BC|          15.2|    2024-01-02|         B|       99.1|         135.2|    22|               99.1|                 135.2|\n",
            "|      P003|Heart Disease| 55|     M|      AB|          12.8|    2024-01-03|         A|       98.9|         142.8|    18|               98.9|                 142.8|\n",
            "|      P004|       Cancer| 38|     F|      ON|           9.3|    2024-01-04|         C|       NULL|         118.3|    12|              98.95|                 118.3|\n",
            "|      P005|     Diabetes| 71|     M|      QC|          14.7|    2024-01-05|         B|       99.3|          NULL|    20|               99.3|    129.64999999999998|\n",
            "|      P006|Heart Disease| 48|     F|      ON|          11.2|    2024-01-06|         A|       98.8|         128.5|    16|               98.8|                 128.5|\n",
            "|      P007|       Cancer| 59|     M|      BC|          13.5|    2024-01-07|         B|       NULL|         138.9|    19|              98.95|                 138.9|\n",
            "|      P008|     Diabetes| 42|     F|      AB|           8.9|    2024-01-08|         C|       99.0|         122.4|    14|               99.0|                 122.4|\n",
            "|      P009|Heart Disease| 65|     M|      QC|          16.4|    2024-01-09|         A|       99.2|          NULL|    25|               99.2|    129.64999999999998|\n",
            "|      P010|       Cancer| 52|     F|      ON|          12.1|    2024-01-10|         B|       98.7|         130.6|    17|               98.7|                 130.6|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+-------------------+----------------------+\n",
            "\n",
            "\n",
            "Rolling Window Analysis (7-day moving average of temperature):\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+--------------------------+\n",
            "|patient_id|    diagnosis|age|gender|province|length_of_stay|admission_date|care_level|temperature|blood_pressure|visits|temperature_rolling_mean_7|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+--------------------------+\n",
            "|      P001|       Cancer| 45|     M|      ON|          10.5|    2024-01-01|         A|       98.6|         120.5|    15|                      98.6|\n",
            "|      P002|     Diabetes| 62|     F|      BC|          15.2|    2024-01-02|         B|       99.1|         135.2|    22|                     98.85|\n",
            "|      P003|Heart Disease| 55|     M|      AB|          12.8|    2024-01-03|         A|       98.9|         142.8|    18|         98.86666666666667|\n",
            "|      P004|       Cancer| 38|     F|      ON|           9.3|    2024-01-04|         C|       NULL|         118.3|    12|         98.86666666666667|\n",
            "|      P005|     Diabetes| 71|     M|      QC|          14.7|    2024-01-05|         B|       99.3|          NULL|    20|         98.97500000000001|\n",
            "|      P006|Heart Disease| 48|     F|      ON|          11.2|    2024-01-06|         A|       98.8|         128.5|    16|         98.94000000000001|\n",
            "|      P007|       Cancer| 59|     M|      BC|          13.5|    2024-01-07|         B|       NULL|         138.9|    19|         98.94000000000001|\n",
            "|      P008|     Diabetes| 42|     F|      AB|           8.9|    2024-01-08|         C|       99.0|         122.4|    14|         99.02000000000001|\n",
            "|      P009|Heart Disease| 65|     M|      QC|          16.4|    2024-01-09|         A|       99.2|          NULL|    25|         99.03999999999999|\n",
            "|      P010|       Cancer| 52|     F|      ON|          12.1|    2024-01-10|         B|       98.7|         130.6|    17|                      99.0|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+--------------------------+\n",
            "\n",
            "\n",
            "Conditional Column Creation:\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+---------+\n",
            "|patient_id|    diagnosis|age|gender|province|length_of_stay|admission_date|care_level|temperature|blood_pressure|visits|age_group|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+---------+\n",
            "|      P001|       Cancer| 45|     M|      ON|          10.5|    2024-01-01|         A|       98.6|         120.5|    15|  'Adult'|\n",
            "|      P002|     Diabetes| 62|     F|      BC|          15.2|    2024-01-02|         B|       99.1|         135.2|    22|  'Adult'|\n",
            "|      P003|Heart Disease| 55|     M|      AB|          12.8|    2024-01-03|         A|       98.9|         142.8|    18|  'Adult'|\n",
            "|      P004|       Cancer| 38|     F|      ON|           9.3|    2024-01-04|         C|       NULL|         118.3|    12|  'Adult'|\n",
            "|      P005|     Diabetes| 71|     M|      QC|          14.7|    2024-01-05|         B|       99.3|          NULL|    20| 'Senior'|\n",
            "|      P006|Heart Disease| 48|     F|      ON|          11.2|    2024-01-06|         A|       98.8|         128.5|    16|  'Adult'|\n",
            "|      P007|       Cancer| 59|     M|      BC|          13.5|    2024-01-07|         B|       NULL|         138.9|    19|  'Adult'|\n",
            "|      P008|     Diabetes| 42|     F|      AB|           8.9|    2024-01-08|         C|       99.0|         122.4|    14|  'Adult'|\n",
            "|      P009|Heart Disease| 65|     M|      QC|          16.4|    2024-01-09|         A|       99.2|          NULL|    25| 'Senior'|\n",
            "|      P010|       Cancer| 52|     F|      ON|          12.1|    2024-01-10|         B|       98.7|         130.6|    17|  'Adult'|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+---------+\n",
            "\n",
            "\n",
            "Dataset Summary:\n",
            "Schema:\n",
            "root\n",
            " |-- patient_id: string (nullable = false)\n",
            " |-- diagnosis: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- province: string (nullable = true)\n",
            " |-- length_of_stay: double (nullable = true)\n",
            " |-- admission_date: string (nullable = true)\n",
            " |-- care_level: string (nullable = true)\n",
            " |-- temperature: double (nullable = true)\n",
            " |-- blood_pressure: double (nullable = true)\n",
            " |-- visits: integer (nullable = true)\n",
            "\n",
            "\n",
            "Sample Data:\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "|patient_id|    diagnosis|age|gender|province|length_of_stay|admission_date|care_level|temperature|blood_pressure|visits|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "|      P001|       Cancer| 45|     M|      ON|          10.5|    2024-01-01|         A|       98.6|         120.5|    15|\n",
            "|      P002|     Diabetes| 62|     F|      BC|          15.2|    2024-01-02|         B|       99.1|         135.2|    22|\n",
            "|      P003|Heart Disease| 55|     M|      AB|          12.8|    2024-01-03|         A|       98.9|         142.8|    18|\n",
            "|      P004|       Cancer| 38|     F|      ON|           9.3|    2024-01-04|         C|       NULL|         118.3|    12|\n",
            "|      P005|     Diabetes| 71|     M|      QC|          14.7|    2024-01-05|         B|       99.3|          NULL|    20|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Row Count: 10\n",
            "\n",
            "Column Summary:\n",
            "+------------+-----+\n",
            "|column_value|count|\n",
            "+------------+-----+\n",
            "|        P003|    1|\n",
            "|        P004|    1|\n",
            "|        P002|    1|\n",
            "|        P001|    1|\n",
            "|        P005|    1|\n",
            "+------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------------+-----+\n",
            "| column_value|count|\n",
            "+-------------+-----+\n",
            "|Heart Disease|    3|\n",
            "|     Diabetes|    3|\n",
            "|       Cancer|    4|\n",
            "+-------------+-----+\n",
            "\n",
            "+------------+-----+\n",
            "|column_value|count|\n",
            "+------------+-----+\n",
            "|          55|    1|\n",
            "|          45|    1|\n",
            "|          38|    1|\n",
            "|          62|    1|\n",
            "|          71|    1|\n",
            "+------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------+-----+\n",
            "|column_value|count|\n",
            "+------------+-----+\n",
            "|           F|    5|\n",
            "|           M|    5|\n",
            "+------------+-----+\n",
            "\n",
            "+------------+-----+\n",
            "|column_value|count|\n",
            "+------------+-----+\n",
            "|          QC|    2|\n",
            "|          BC|    2|\n",
            "|          ON|    4|\n",
            "|          AB|    2|\n",
            "+------------+-----+\n",
            "\n",
            "+------------+-----+\n",
            "|column_value|count|\n",
            "+------------+-----+\n",
            "|        12.8|    1|\n",
            "|         9.3|    1|\n",
            "|        15.2|    1|\n",
            "|        14.7|    1|\n",
            "|        10.5|    1|\n",
            "+------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------+-----+\n",
            "|column_value|count|\n",
            "+------------+-----+\n",
            "|  2024-01-02|    1|\n",
            "|  2024-01-04|    1|\n",
            "|  2024-01-03|    1|\n",
            "|  2024-01-05|    1|\n",
            "|  2024-01-01|    1|\n",
            "+------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------+-----+\n",
            "|column_value|count|\n",
            "+------------+-----+\n",
            "|           B|    4|\n",
            "|           C|    2|\n",
            "|           A|    4|\n",
            "+------------+-----+\n",
            "\n",
            "+------------+-----+\n",
            "|column_value|count|\n",
            "+------------+-----+\n",
            "|        NULL|    2|\n",
            "|        98.9|    1|\n",
            "|        99.1|    1|\n",
            "|        99.3|    1|\n",
            "|        98.6|    1|\n",
            "+------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------+-----+\n",
            "|column_value|count|\n",
            "+------------+-----+\n",
            "|        NULL|    2|\n",
            "|       142.8|    1|\n",
            "|       118.3|    1|\n",
            "|       135.2|    1|\n",
            "|       120.5|    1|\n",
            "+------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------+-----+\n",
            "|column_value|count|\n",
            "+------------+-----+\n",
            "|          12|    1|\n",
            "|          22|    1|\n",
            "|          20|    1|\n",
            "|          15|    1|\n",
            "|          18|    1|\n",
            "+------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Split Dataset into Training and Testing:\n",
            "Training Data Sample:\n",
            "+----------+---------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "|patient_id|diagnosis|age|gender|province|length_of_stay|admission_date|care_level|temperature|blood_pressure|visits|\n",
            "+----------+---------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "|      P001|   Cancer| 45|     M|      ON|          10.5|    2024-01-01|         A|       98.6|         120.5|    15|\n",
            "|      P002| Diabetes| 62|     F|      BC|          15.2|    2024-01-02|         B|       99.1|         135.2|    22|\n",
            "|      P004|   Cancer| 38|     F|      ON|           9.3|    2024-01-04|         C|       NULL|         118.3|    12|\n",
            "|      P005| Diabetes| 71|     M|      QC|          14.7|    2024-01-05|         B|       99.3|          NULL|    20|\n",
            "|      P007|   Cancer| 59|     M|      BC|          13.5|    2024-01-07|         B|       NULL|         138.9|    19|\n",
            "+----------+---------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Testing Data Sample:\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "|patient_id|    diagnosis|age|gender|province|length_of_stay|admission_date|care_level|temperature|blood_pressure|visits|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "|      P003|Heart Disease| 55|     M|      AB|          12.8|    2024-01-03|         A|       98.9|         142.8|    18|\n",
            "|      P006|Heart Disease| 48|     F|      ON|          11.2|    2024-01-06|         A|       98.8|         128.5|    16|\n",
            "+----------+-------------+---+------+--------+--------------+--------------+----------+-----------+--------------+------+\n",
            "\n",
            "\n",
            "Time Series Forecasting (placeholder):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".............\n",
            "----------------------------------------------------------------------\n",
            "Ran 13 tests in 22.171s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    }
  ]
}